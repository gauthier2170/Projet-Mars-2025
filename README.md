# üí° Projet GPT-2 - Fine-tuning en fran√ßais

Ce projet montre comment fine-tuner GPT-2 sur des textes en fran√ßais, √† l'aide de Hugging Face.

---

## üîç Partie 1 : Chargement et pr√©paration des donn√©es

On utilise ici le dataset `rayml/french_gutenberg`, qui contient des livres traduits en fran√ßais issus du projet Gutenberg.

---

### üß™ Installation des biblioth√®ques n√©cessaires

```bash
!pip install transformers torch

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_name = "gpt2-medium"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name)
model.eval()
model.to("cuda" if torch.cuda.is_available() else "cpu")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

examples = [
    "What is the capital of France?",
    "What are the three primary colors?",
    "What does DNA stand for?"
]

for instruction in examples:
    prompt = f"Instruction: {instruction}\nR√©ponse:"
    output = generator(prompt, max_new_tokens=60)[0]["generated_text"]
    print("\n", instruction)
    print(output)
