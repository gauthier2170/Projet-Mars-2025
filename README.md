# ğŸ’¡ Projet GPT-2 - Fine-tuning en franÃ§ais

Ce projet montre comment fine-tuner GPT-2 sur des textes en franÃ§ais, Ã  l'aide de Hugging Face.

---

## ğŸ” Partie 1 : Chargement et prÃ©paration des donnÃ©es

On utilise ici le dataset `rayml/french_gutenberg`, qui contient des livres traduits en franÃ§ais issus du projet Gutenberg.

---

### ğŸ§ª Installation des bibliothÃ¨ques nÃ©cessaires

```bash
pip install torch datasets transformers tqdm matplotlib
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def tokenize(element):
    return tokenizer(element["text"])

dataset = dataset.map(tokenize, batched=True, remove_columns=["text"])
def group_texts(examples):
    concatenated = sum(examples["input_ids"], [])
    total_len = (len(concatenated) // SEQUENCE_LENGTH) * SEQUENCE_LENGTH
    result = [
        concatenated[i : i + SEQUENCE_LENGTH]
        for i in range(0, total_len, SEQUENCE_LENGTH)
    ]
    return {"input_ids": result, "labels": result}

dataset = dataset.map(group_texts, batched=True)
