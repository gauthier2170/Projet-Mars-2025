# ğŸ’¡ Projet GPT-2 - Fine-tuning en franÃ§ais

Ce projet montre comment fine-tuner GPT-2 sur des textes en franÃ§ais, Ã  l'aide de Hugging Face.

---

## ğŸ” Partie 1 : Chargement et prÃ©paration des donnÃ©es

On utilise ici le dataset `rayml/french_gutenberg`, qui contient des livres traduits en franÃ§ais issus du projet Gutenberg.

---

### ğŸ§ª Installation des bibliothÃ¨ques nÃ©cessaires

```bash
!pip install transformers torch

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_name = "gpt2-medium"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name)
model.eval()
model.to("cuda" if torch.cuda.is_available() else "cpu")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

examples = [
    "What is the capital of France?",
    "What are the three primary colors?",
    "What does DNA stand for?"
]

for instruction in examples:
    prompt = f"Instruction: {instruction}\nRÃ©ponse:"
    output = generator(prompt, max_new_tokens=60)[0]["generated_text"]
    print("\n", instruction)
    print(output)
```
# ğŸ’¡ Projet GPT-2 - Fine-tuning en franÃ§ais

Ce projet montre comment fine-tuner GPT-2 sur des textes en franÃ§ais, Ã  l'aide de Hugging Face.

---

## ğŸ“„ TÃ©lÃ©charger le code en PDF

ğŸ‘‰ [Clique ici pour ouvrir le PDF contenant le code](1ER%20partie%20gpt-2.pdf)

---

## ğŸ” Partie 1 : Chargement et prÃ©paration des donnÃ©es

### ğŸ§ª Installation

```bash
!pip install torch datasets transformers tqdm matplotlib

